{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eaa9344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dsta0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "c:\\Users\\dsta0\\anaconda3\\envs\\py310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import re\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.svm import SVC\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import spacy\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6edf71e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##df_tweets = pd.read_csv('training.1600000.processed.noemoticon.csv', \n",
    "#df_tweets = pd.read_csv('balanced.csv', \n",
    "# df_tweets = pd.read_csv('balanced_sentiment_dataset_100k.csv', \n",
    "#                     encoding='latin-1',  names=['target','ids','date','flag','user','text'])\n",
    "\n",
    "df_tweets = pd.read_csv('balanced_sentiment_dataset_100k.csv', \n",
    "                    encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb5eb114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "target",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ids",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "flag",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "user",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "2c89eecf-a14e-4f26-8c93-10da3ded20fc",
       "rows": [
        [
         "80635",
         "0",
         "2067588480",
         "Sun Jun 07 12:36:40 PDT 2009",
         "NO_QUERY",
         "knunn",
         "forgot to checkin at 11:15   - http://bkite.com/08hb9"
        ],
        [
         "46195",
         "0",
         "2201323585",
         "Tue Jun 16 20:06:57 PDT 2009",
         "NO_QUERY",
         "Kelndan",
         "I WANT A DSi sooooooomone..........why cant i be bloody rich......all i seem to win is a bloody free cherry ripe "
        ],
        [
         "32206",
         "1",
         "2015098343",
         "Wed Jun 03 03:41:40 PDT 2009",
         "NO_QUERY",
         "WParenthetical",
         "&quot;If a triangle could speak it would say that God is eminently triangular&quot; -Spinoza "
        ],
        [
         "18652",
         "1",
         "2180956090",
         "Mon Jun 15 11:10:11 PDT 2009",
         "NO_QUERY",
         "Thewie72",
         "@ninaoo7 use #captions  I copied it so I can paste it in any message x"
        ],
        [
         "57037",
         "1",
         "1881000762",
         "Fri May 22 03:15:13 PDT 2009",
         "NO_QUERY",
         "LesleySmith",
         "@louisebolotin I need me one of those "
        ],
        [
         "94104",
         "1",
         "1833565154",
         "Sun May 17 23:42:00 PDT 2009",
         "NO_QUERY",
         "charliechap",
         "@MsKae Sleep well, and don't forget to air out the dog! "
        ],
        [
         "54644",
         "0",
         "2286970675",
         "Mon Jun 22 17:10:30 PDT 2009",
         "NO_QUERY",
         "bethanycamille",
         "Sad! I Got blue contacts and you can not even tell I have them on! "
        ],
        [
         "69510",
         "1",
         "2065942091",
         "Sun Jun 07 09:35:21 PDT 2009",
         "NO_QUERY",
         "ReneeDeLuca",
         "@MelissaEGilbert Will have to read your book--I am your age, and related to you so much as a kid, I thought for sure we'd be friends "
        ],
        [
         "67635",
         "1",
         "1685888650",
         "Sun May 03 02:36:55 PDT 2009",
         "NO_QUERY",
         "daanwesterink",
         "My brother is coming over. Nothing else matters "
        ],
        [
         "6603",
         "0",
         "2262977193",
         "Sat Jun 20 23:39:52 PDT 2009",
         "NO_QUERY",
         "JuLars",
         "I am pissed my sleep pattern is so off  I miss going to bed at 9 and waking up early..."
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80635</th>\n",
       "      <td>0</td>\n",
       "      <td>2067588480</td>\n",
       "      <td>Sun Jun 07 12:36:40 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>knunn</td>\n",
       "      <td>forgot to checkin at 11:15   - http://bkite.co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46195</th>\n",
       "      <td>0</td>\n",
       "      <td>2201323585</td>\n",
       "      <td>Tue Jun 16 20:06:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Kelndan</td>\n",
       "      <td>I WANT A DSi sooooooomone..........why cant i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32206</th>\n",
       "      <td>1</td>\n",
       "      <td>2015098343</td>\n",
       "      <td>Wed Jun 03 03:41:40 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>WParenthetical</td>\n",
       "      <td>&amp;quot;If a triangle could speak it would say t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18652</th>\n",
       "      <td>1</td>\n",
       "      <td>2180956090</td>\n",
       "      <td>Mon Jun 15 11:10:11 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Thewie72</td>\n",
       "      <td>@ninaoo7 use #captions  I copied it so I can p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57037</th>\n",
       "      <td>1</td>\n",
       "      <td>1881000762</td>\n",
       "      <td>Fri May 22 03:15:13 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>LesleySmith</td>\n",
       "      <td>@louisebolotin I need me one of those</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94104</th>\n",
       "      <td>1</td>\n",
       "      <td>1833565154</td>\n",
       "      <td>Sun May 17 23:42:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>charliechap</td>\n",
       "      <td>@MsKae Sleep well, and don't forget to air out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54644</th>\n",
       "      <td>0</td>\n",
       "      <td>2286970675</td>\n",
       "      <td>Mon Jun 22 17:10:30 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bethanycamille</td>\n",
       "      <td>Sad! I Got blue contacts and you can not even ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69510</th>\n",
       "      <td>1</td>\n",
       "      <td>2065942091</td>\n",
       "      <td>Sun Jun 07 09:35:21 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ReneeDeLuca</td>\n",
       "      <td>@MelissaEGilbert Will have to read your book--...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67635</th>\n",
       "      <td>1</td>\n",
       "      <td>1685888650</td>\n",
       "      <td>Sun May 03 02:36:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>daanwesterink</td>\n",
       "      <td>My brother is coming over. Nothing else matters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6603</th>\n",
       "      <td>0</td>\n",
       "      <td>2262977193</td>\n",
       "      <td>Sat Jun 20 23:39:52 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>JuLars</td>\n",
       "      <td>I am pissed my sleep pattern is so off  I miss...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      target         ids                          date      flag  \\\n",
       "80635      0  2067588480  Sun Jun 07 12:36:40 PDT 2009  NO_QUERY   \n",
       "46195      0  2201323585  Tue Jun 16 20:06:57 PDT 2009  NO_QUERY   \n",
       "32206      1  2015098343  Wed Jun 03 03:41:40 PDT 2009  NO_QUERY   \n",
       "18652      1  2180956090  Mon Jun 15 11:10:11 PDT 2009  NO_QUERY   \n",
       "57037      1  1881000762  Fri May 22 03:15:13 PDT 2009  NO_QUERY   \n",
       "94104      1  1833565154  Sun May 17 23:42:00 PDT 2009  NO_QUERY   \n",
       "54644      0  2286970675  Mon Jun 22 17:10:30 PDT 2009  NO_QUERY   \n",
       "69510      1  2065942091  Sun Jun 07 09:35:21 PDT 2009  NO_QUERY   \n",
       "67635      1  1685888650  Sun May 03 02:36:55 PDT 2009  NO_QUERY   \n",
       "6603       0  2262977193  Sat Jun 20 23:39:52 PDT 2009  NO_QUERY   \n",
       "\n",
       "                 user                                               text  \n",
       "80635           knunn  forgot to checkin at 11:15   - http://bkite.co...  \n",
       "46195         Kelndan  I WANT A DSi sooooooomone..........why cant i ...  \n",
       "32206  WParenthetical  &quot;If a triangle could speak it would say t...  \n",
       "18652        Thewie72  @ninaoo7 use #captions  I copied it so I can p...  \n",
       "57037     LesleySmith             @louisebolotin I need me one of those   \n",
       "94104     charliechap  @MsKae Sleep well, and don't forget to air out...  \n",
       "54644  bethanycamille  Sad! I Got blue contacts and you can not even ...  \n",
       "69510     ReneeDeLuca  @MelissaEGilbert Will have to read your book--...  \n",
       "67635   daanwesterink   My brother is coming over. Nothing else matters   \n",
       "6603           JuLars  I am pissed my sleep pattern is so off  I miss...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d23ed6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = df_tweets[['target', 'text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02e0687",
   "metadata": {},
   "source": [
    "# Basic Preprocessing (To be deleted later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9978cba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs, mentions, and special chars\n",
    "    text = re.sub(r'http\\S+|@\\w+|[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize and remove stopwords/stem\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df_tweets['cleaned_text'] = df_tweets['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b96bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tweets['target'] = df_tweets['target'].replace(4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e17b45ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.to_csv('sample_tweets_cleaned.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0def11",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2855114",
   "metadata": {},
   "source": [
    "Handle the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c5bc2348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target    0\n",
      "text      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_percentage = df_tweets.isna().sum()\n",
    "print(missing_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "80f4c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['lowercase'] = df_tweets['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3751c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "df_tweets['cleaned'] = df_tweets['lowercase'].apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "32da547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SpaCy language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Make sure it's installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9f5cbcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "df_tweets['no_stopwords'] = df_tweets['cleaned'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "821e84c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokens(tokens):\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "df_tweets['lemmatized'] = df_tweets['no_stopwords'].apply(lemmatize_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "083c44ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hashtags(tokens):\n",
    "    return [token[1:] if token.startswith('#') else token for token in tokens]\n",
    "\n",
    "df_tweets['cleaned_hashtags'] = df_tweets['lemmatized'].apply(clean_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a266c273",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "def remove_emojis(tokens):\n",
    "    return [emoji_pattern.sub(r'', token) for token in tokens if not emoji_pattern.match(token)]\n",
    "\n",
    "df_tweets['no_emojis'] = df_tweets['cleaned_hashtags'].apply(remove_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d7f5d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.to_csv('100k_tweets_preprocessed.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca806de",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ec3649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_cleaned = pd.read_csv(\"100k_tweets_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcbead50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sentiment",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "no_emojis",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "ad88e139-782d-4cc6-818b-bfb26d46a6ff",
       "rows": [
        [
         "23950",
         "0",
         "['listen', 'radio', '    ', 'not', 'ticket', '  ', 'live', 'swindon', 'gutted', '  ', 'excited', 'hear', 'neyo', 'ndubz']"
        ],
        [
         "23091",
         "0",
         "['mom', 'watch', 'cnbcbore']"
        ],
        [
         "1539",
         "0",
         "['summer', 'oh', 'summer', 'm', 'read', 'midnight']"
        ],
        [
         "19091",
         "0",
         "['tracitoguchi', 'melissa', 'hope', 'little', 'oven', 'clear', 'cone', '  ', 'gt', 'ltinsert', 'suspense', 'music', 'heregt', 'lol', 'thank']"
        ],
        [
         "84056",
         "1",
         "['rizzn', 'loss', '  ', 'point', 'waste', 'time', 'right', '  ', 'fun']"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>no_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23950</th>\n",
       "      <td>0</td>\n",
       "      <td>['listen', 'radio', '    ', 'not', 'ticket', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23091</th>\n",
       "      <td>0</td>\n",
       "      <td>['mom', 'watch', 'cnbcbore']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>0</td>\n",
       "      <td>['summer', 'oh', 'summer', 'm', 'read', 'midni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19091</th>\n",
       "      <td>0</td>\n",
       "      <td>['tracitoguchi', 'melissa', 'hope', 'little', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84056</th>\n",
       "      <td>1</td>\n",
       "      <td>['rizzn', 'loss', '  ', 'point', 'waste', 'tim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                          no_emojis\n",
       "23950          0  ['listen', 'radio', '    ', 'not', 'ticket', '...\n",
       "23091          0                       ['mom', 'watch', 'cnbcbore']\n",
       "1539           0  ['summer', 'oh', 'summer', 'm', 'read', 'midni...\n",
       "19091          0  ['tracitoguchi', 'melissa', 'hope', 'little', ...\n",
       "84056          1  ['rizzn', 'loss', '  ', 'point', 'waste', 'tim..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets_cleaned = df_tweets_cleaned[['sentiment', 'no_emojis']]\n",
    "\n",
    "df_tweets_cleaned.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797afa16",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee75ac1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                        ['stargazer', 's', 'awesome']\n",
       "1                   ['cunningstunt', '  ', 'yes', ' ']\n",
       "2                              ['n', 'bed', 'aaallll']\n",
       "3    ['thorney', 'guinea', 'fowl', 've', 'hear', 'l...\n",
       "4                     ['def', 'go', 'movie', 'tonite']\n",
       "Name: no_emojis, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_tweets_cleaned['no_emojis']\n",
    "y = df_tweets_cleaned['sentiment']\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afe9508d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80000,), (20000,), (80000,), (20000,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training (80%) | Testing (20%) -> TF-IDF\n",
    "X_train_tf_idf, X_test_tf_idf, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "X_train_tf_idf.shape, X_test_tf_idf.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c9ae2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = TfidfVectorizer()\n",
    "\n",
    "# X_tf_idf = featurizer.fit_transform(X)\n",
    "X_train_tf_idf = featurizer.fit_transform(X_train_tf_idf)\n",
    "X_test_tf_idf  = featurizer.transform(X_test_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8698361b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<80000x84479 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 523356 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_tf_idf #just ot showed the result (to be deleted later)\n",
    "X_train_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2325ccfc",
   "metadata": {},
   "source": [
    "## Word2Vec (word embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "116d4d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize for Word2Vec\n",
    "sentences = [text.split() for text in df_tweets_cleaned['no_emojis']]\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Document embedding (average word vectors)\n",
    "def document_vector(text):\n",
    "    words = text.split()\n",
    "    return np.mean([w2v_model.wv[word] for word in words if word in w2v_model.wv], axis=0)\n",
    "\n",
    "X_w2v = np.array([document_vector(text) for text in df_tweets_cleaned['no_emojis']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d91bece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.30063775,  0.86573005,  0.74986476, ..., -0.4213965 ,\n",
       "         0.3709987 ,  0.09516541],\n",
       "       [-0.65612286,  1.0883988 ,  0.87931824, ..., -0.7189896 ,\n",
       "         0.39998937,  0.3961855 ],\n",
       "       [-0.13225685,  0.4803958 ,  0.14354837, ..., -0.19197087,\n",
       "        -0.14579284, -0.32398987],\n",
       "       ...,\n",
       "       [-0.29141712,  0.8073322 ,  0.6703043 , ..., -0.57488894,\n",
       "         0.2615127 , -0.04208249],\n",
       "       [-0.5910112 ,  0.72747535,  0.34523973, ..., -0.5053299 ,\n",
       "         0.20441668, -0.3313579 ],\n",
       "       [-0.58046764,  0.88760567,  0.6213814 , ..., -0.60579216,\n",
       "         0.26138896,  0.08378785]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_w2v #just ot showed the result (to be deleted later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392c329e",
   "metadata": {},
   "source": [
    "## BERT (contextual embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd995e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3125/3125 [05:30<00:00,  9.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# Lightweight BERT-based model (to be executed locally)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "\n",
    "# Ensure cleaned text is a list of strings\n",
    "texts = df_tweets_cleaned['no_emojis'].fillna('').astype(str).tolist()\n",
    "\n",
    "# Generate embeddings\n",
    "X_bert = model.encode(texts, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23112ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04485021, -0.04264412, -0.00525744, ..., -0.00553362,\n",
       "        -0.00602297,  0.02970693],\n",
       "       [-0.02060345,  0.03405784, -0.02794828, ...,  0.11837119,\n",
       "         0.03944125, -0.02530853],\n",
       "       [ 0.04897955, -0.03182269,  0.03071005, ...,  0.05528144,\n",
       "         0.01947791, -0.0520927 ],\n",
       "       ...,\n",
       "       [-0.04037578, -0.07581542, -0.02984781, ...,  0.07661375,\n",
       "         0.00486614,  0.03132267],\n",
       "       [-0.018034  ,  0.00310953,  0.03367927, ...,  0.05459262,\n",
       "         0.02036844,  0.05526228],\n",
       "       [-0.03944533, -0.09184176, -0.01866939, ...,  0.0556329 ,\n",
       "        -0.00763105,  0.0256989 ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bert #just ot showed the result (to be deleted later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6b5386",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f842428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# SVM using TF-IDF \n",
    "svm_td_idf = SVC(kernel='rbf', C=1.0)\n",
    "\n",
    "# TF-IDF SVM Training\n",
    "svm_td_idf.fit(X_train_tf_idf, y_train)\n",
    "print(\"Done!\")\n",
    "# y_pred_tfidf = svm_td_idf.predict(X_test_tf_idf)\n",
    "# print(\"TF-IDF Accuracy:\", accuracy_score(y_test, y_pred_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58f10b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80000, 100), (20000, 100), (80000,), (20000,))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training (80%) | Testing (20%) -> Word2Vec\n",
    "X_train_w2v, X_test_w2v, y_train, y_test = train_test_split(X_w2v, y, test_size=0.2, random_state=123) # same seed as tf-idf split\n",
    "\n",
    "X_train_w2v.shape, X_test_w2v.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15d55b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# SVM using Word2Vec\n",
    "svm_w2v = SVC(kernel='rbf', C=1.0)\n",
    "\n",
    "# Word2Vec SVM Training\n",
    "svm_w2v.fit(X_train_w2v, y_train)\n",
    "print(\"Done!\")\n",
    "# y_pred_w2v = svm_w2v.predict(X_test_w2v)\n",
    "# print(\"Word2Vec Accuracy:\", accuracy_score(y_test, y_pred_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8582f173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80000, 384), (20000, 384), (80000,), (20000,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training (80%) | Testing (20%) -> BERT\n",
    "X_train_bert, X_test_bert, y_train, y_test = train_test_split(X_bert, y, test_size=0.2, random_state=123) # same seed as tf-idf split\n",
    "\n",
    "X_train_bert.shape, X_test_bert.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b490165c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# SVM using BERT\n",
    "svm_bert = SVC(kernel='rbf', C=1.0)\n",
    "\n",
    "# BERT SVM Training\n",
    "svm_bert.fit(X_train_bert, y_train)\n",
    "print(\"Done!\")\n",
    "# y_pred_bert = svm_bert.predict(X_test_bert)\n",
    "# print(\"BERT Accuracy:\", accuracy_score(y_test, y_pred_bert))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1c8082",
   "metadata": {},
   "source": [
    "# Feature Extration Techniques' Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed963624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Accuracy: 0.7615\n",
      "\n",
      "TF-IDF Confusion Matrix:\n",
      " [[7451 2485]\n",
      " [2285 7779]]\n",
      "\n",
      "TF-IDF Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.75      0.76      9936\n",
      "           1       0.76      0.77      0.77     10064\n",
      "\n",
      "    accuracy                           0.76     20000\n",
      "   macro avg       0.76      0.76      0.76     20000\n",
      "weighted avg       0.76      0.76      0.76     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf_pred = svm_td_idf.predict(X_test_tf_idf)\n",
    "\n",
    "# Evaluating TF-IDF\n",
    "tf_idf_accuracy = accuracy_score(y_test, tf_idf_pred)\n",
    "tf_idf_cm = confusion_matrix(y_test, tf_idf_pred)\n",
    "tf_idf_report = classification_report(y_test, tf_idf_pred)\n",
    "\n",
    "print(\"TF-IDF Accuracy:\", tf_idf_accuracy)\n",
    "print(\"\\nTF-IDF Confusion Matrix:\\n\", tf_idf_cm)\n",
    "print(\"\\nTF-IDF Classification Report:\\n\", tf_idf_report)\n",
    "\n",
    "# print(confusion_matrix(y_test, y_pred_tfidf))\n",
    "# print(classification_report(y_test, y_pred_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a3ba396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Accuracy: 0.66845\n",
      "\n",
      "Word2Vec Confusion Matrix:\n",
      " [[6461 3475]\n",
      " [3156 6908]]\n",
      "\n",
      "Word2Vec Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.65      0.66      9936\n",
      "           1       0.67      0.69      0.68     10064\n",
      "\n",
      "    accuracy                           0.67     20000\n",
      "   macro avg       0.67      0.67      0.67     20000\n",
      "weighted avg       0.67      0.67      0.67     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w2v_pred = svm_w2v.predict(X_test_w2v)\n",
    "\n",
    "# Evaluating Word2Vec\n",
    "w2v_accuracy = accuracy_score(y_test, w2v_pred)\n",
    "w2v_cm = confusion_matrix(y_test, w2v_pred)\n",
    "w2v_report = classification_report(y_test, w2v_pred)\n",
    "\n",
    "print(\"Word2Vec Accuracy:\", w2v_accuracy)\n",
    "print(\"\\nWord2Vec Confusion Matrix:\\n\", w2v_cm)\n",
    "print(\"\\nWord2Vec Classification Report:\\n\", w2v_report)\n",
    "\n",
    "# print(confusion_matrix(y_test, y_pred_w2v))\n",
    "# print(classification_report(y_test, y_pred_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f514f272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Accuracy: 0.7511\n",
      "\n",
      "BERT Confusion Matrix:\n",
      " [[7261 2675]\n",
      " [2303 7761]]\n",
      "\n",
      "BERT Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.73      0.74      9936\n",
      "           1       0.74      0.77      0.76     10064\n",
      "\n",
      "    accuracy                           0.75     20000\n",
      "   macro avg       0.75      0.75      0.75     20000\n",
      "weighted avg       0.75      0.75      0.75     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_pred = svm_bert.predict(X_test_bert)\n",
    "\n",
    "# Evaluating BERT\n",
    "bert_accuracy = accuracy_score(y_test, bert_pred)\n",
    "bert_cm = confusion_matrix(y_test, bert_pred)\n",
    "bert_report = classification_report(y_test, bert_pred)\n",
    "\n",
    "print(\"BERT Accuracy:\", bert_accuracy)\n",
    "print(\"\\nBERT Confusion Matrix:\\n\", bert_cm)\n",
    "print(\"\\nBERT Classification Report:\\n\", bert_report)\n",
    "\n",
    "# Evaluating BERT\n",
    "\n",
    "# print(confusion_matrix(y_test, y_pred_bert))\n",
    "# print(classification_report(y_test, y_pred_bert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939c1fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = {\n",
    "    'TF-IDF': accuracy_score(y_test, tf_idf_pred),\n",
    "    'Word2Vec': accuracy_score(y_test, w2v_pred),\n",
    "    'BERT': accuracy_score(y_test, bert_pred)\n",
    "}\n",
    "\n",
    "plt.bar(accuracies.keys(), accuracies.values())\n",
    "plt.title('SVM Accuracy by Feature Extraction Method')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c903dc",
   "metadata": {},
   "source": [
    "# Deep Learning Approach\n",
    "The experiment showed that it improves the accuracy in just ~1% using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6585769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 128)               12928     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,249\n",
      "Trainable params: 21,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Classification using DNN\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(100,)),  # Input dim = 100 (Word2Vec vec size)\n",
    "    Dropout(0.5),  # Regularization\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')  # Binary output (0=Negative, 1=Positive)\n",
    "])\n",
    "# model = Sequential([\n",
    "#     # Input layer (100-dim Word2Vec vectors)\n",
    "#     Dense(256, activation='relu', input_shape=(100,), kernel_regularizer=l2(0.01)),\n",
    "#     BatchNormalization(),\n",
    "#     Dropout(0.5),\n",
    "    \n",
    "#     # Hidden layers\n",
    "#     Dense(128, activation='relu', kernel_regularizer=l2(0.005)),\n",
    "#     BatchNormalization(),\n",
    "#     Dropout(0.4),\n",
    "    \n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dropout(0.3),\n",
    "    \n",
    "#     # Output layer\n",
    "#     Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ad86790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2500/2500 [==============================] - 10s 3ms/step - loss: 0.6387 - accuracy: 0.6255 - val_loss: 0.6197 - val_accuracy: 0.6478\n",
      "Epoch 2/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6266 - accuracy: 0.6421 - val_loss: 0.6193 - val_accuracy: 0.6592\n",
      "Epoch 3/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6218 - accuracy: 0.6483 - val_loss: 0.6122 - val_accuracy: 0.6578\n",
      "Epoch 4/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6191 - accuracy: 0.6524 - val_loss: 0.6141 - val_accuracy: 0.6558\n",
      "Epoch 5/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6172 - accuracy: 0.6540 - val_loss: 0.6122 - val_accuracy: 0.6611\n",
      "Epoch 6/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6158 - accuracy: 0.6542 - val_loss: 0.6093 - val_accuracy: 0.6631\n",
      "Epoch 7/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6143 - accuracy: 0.6560 - val_loss: 0.6081 - val_accuracy: 0.6651\n",
      "Epoch 8/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6129 - accuracy: 0.6570 - val_loss: 0.6129 - val_accuracy: 0.6656\n",
      "Epoch 9/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6120 - accuracy: 0.6577 - val_loss: 0.6083 - val_accuracy: 0.6690\n",
      "Epoch 10/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6110 - accuracy: 0.6581 - val_loss: 0.6061 - val_accuracy: 0.6605\n",
      "Epoch 11/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6108 - accuracy: 0.6584 - val_loss: 0.6107 - val_accuracy: 0.6707\n",
      "Epoch 12/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6108 - accuracy: 0.6616 - val_loss: 0.6100 - val_accuracy: 0.6615\n",
      "Epoch 13/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6093 - accuracy: 0.6629 - val_loss: 0.6098 - val_accuracy: 0.6701\n",
      "Epoch 14/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6098 - accuracy: 0.6602 - val_loss: 0.6083 - val_accuracy: 0.6645\n",
      "Epoch 15/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6090 - accuracy: 0.6625 - val_loss: 0.6087 - val_accuracy: 0.6651\n",
      "Epoch 16/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6076 - accuracy: 0.6632 - val_loss: 0.6059 - val_accuracy: 0.6713\n",
      "Epoch 17/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6081 - accuracy: 0.6629 - val_loss: 0.6067 - val_accuracy: 0.6695\n",
      "Epoch 18/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6070 - accuracy: 0.6641 - val_loss: 0.6109 - val_accuracy: 0.6651\n",
      "Epoch 19/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6072 - accuracy: 0.6632 - val_loss: 0.6064 - val_accuracy: 0.6715\n",
      "Epoch 20/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6073 - accuracy: 0.6626 - val_loss: 0.6113 - val_accuracy: 0.6712\n",
      "Epoch 21/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6074 - accuracy: 0.6625 - val_loss: 0.6093 - val_accuracy: 0.6696\n",
      "Epoch 22/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6060 - accuracy: 0.6646 - val_loss: 0.6037 - val_accuracy: 0.6697\n",
      "Epoch 23/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6054 - accuracy: 0.6652 - val_loss: 0.6089 - val_accuracy: 0.6675\n",
      "Epoch 24/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6054 - accuracy: 0.6654 - val_loss: 0.6039 - val_accuracy: 0.6714\n",
      "Epoch 25/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6058 - accuracy: 0.6644 - val_loss: 0.6114 - val_accuracy: 0.6678\n",
      "Epoch 26/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6045 - accuracy: 0.6660 - val_loss: 0.6062 - val_accuracy: 0.6702\n",
      "Epoch 27/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6048 - accuracy: 0.6654 - val_loss: 0.6098 - val_accuracy: 0.6665\n",
      "Epoch 28/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6041 - accuracy: 0.6660 - val_loss: 0.5997 - val_accuracy: 0.6732\n",
      "Epoch 29/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6033 - accuracy: 0.6665 - val_loss: 0.6060 - val_accuracy: 0.6707\n",
      "Epoch 30/30\n",
      "2500/2500 [==============================] - 6s 2ms/step - loss: 0.6034 - accuracy: 0.6668 - val_loss: 0.6033 - val_accuracy: 0.6726\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train_w2v, y_train,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_w2v, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "691ed0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_w2v shape: (35000, 100)\n",
      "y_train shape: (35000,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train_w2v shape: {X_train_w2v.shape}\")  # Should be (n_samples, 100)\n",
    "print(f\"y_train shape: {y_train.shape}\")  # Should be (n_samples,) or (n_samples, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
